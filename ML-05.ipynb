{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee49aaa",
   "metadata": {},
   "source": [
    "## Assignment-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4f177",
   "metadata": {},
   "source": [
    "__Ans 1__:\n",
    "- There are Five core tasks in the common ML workflow:\n",
    "\n",
    "1. Get Data: The first step in the Machine Learning process is getting data.\n",
    "2. Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "3. Train Model: This step is where the magic happens!\n",
    "4. Testing Model.\n",
    "5. Improving model.\n",
    "6. Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "7. Preprocessing involves both data validation and data imputation\n",
    "8. The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "9. The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cf8e1",
   "metadata": {},
   "source": [
    "__Ans 2__:\n",
    "- The Data Type Is Broadly Classified Into:\n",
    "\n",
    "1. __Quantitative__\n",
    "2. __Qualitative__\n",
    "- __Quantitative Data Type__: This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers. E.G., Profit, Quantity Sold, Height, Weight, Temperature, Etc. This Is Again Of Two Types\n",
    "1. __Discrete Data Type__: – The Numeric Data Which Have Discrete Values Or Whole Numbers. This Type Of Variable Value If Expressed In Decimal Format Will Have No Proper Meaning. Their Values Can Be Counted. E.G.: – No. Of Cars You Have, No. Of Marbles In Containers, Students In A Class, Etc.\n",
    "2. __Continuous Data Type__: – The Numerical Measures Which Can Take The Value Within A Certain Range. This Type Of Variable Value If Expressed In Decimal Format Has True Meaning. Their Values Can Not Be Counted But Measured. The Value Can Be Infinite E.G.: Height, Weight, Time, Area, Distance, Measurement Of Rainfall, Etc.\n",
    "- __Qualitative Data Type__: These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type. This Can Be Divided Into:-\n",
    "1. __Structured Data__: This Type Of Data Is Either Number Or Words. This Can Take Numerical Values But Mathematical Operations Cannot Be Performed On It. This Type Of Data Is Expressed In Tabular Format. E.G.) Sunny=1, Cloudy=2, Windy=3 Or Binary Form Data Like 0 Or1, Good Or Bad, Etc.\n",
    "2. __Unstructured Data__: This Type Of Data Does Not Have The Proper Format And Therefore Known As Unstructured Data.This Comprises Textual Data, Sounds, Images, Videos, Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85499e3a",
   "metadata": {},
   "source": [
    "__Ans 3__:\n",
    "- The following is a basic data collection that includes some sample records.\n",
    "\n",
    "1. __Determine What Information You Want to Collect__: The first thing you need to do is choose what details you want to collect. You’ll need to decide what topics the information will cover, who you want to collect it from and how much data you need. Your goals — what you hope to accomplish using your data — will determine your answers to these questions. As an example, you may decide to collect data about which type of articles are most popular on your website among visitors who are between the ages of 18 and 34. You might also choose to gather information about the average age of all of the customers who bought a product from your company within the last month.\n",
    "\n",
    "2. __Set a Timeframe for Data Collection__: Next, you can start formulating your plan for how you’ll collect your data. In the early stages of your planning process, you should establish a timeframe for your data collection. You may want to gather some types of data continuously. When it comes to transactional data and website visitor data, for example, you may want to set up a method for tracking that data over the long term. If you’re tracking data for a specific campaign, however, you’ll track it over a defined period. In these instances, you’ll have a schedule for when you’ll start and end your data collection.\n",
    "\n",
    "3. __Determine Your Data Collection Method__: At this step, you will choose the data collection method that will make up the core of your data-gathering strategy. To select the right collection method, you’ll need to consider the type of information you want to collect, the timeframe over which you’ll obtain it and the other aspects you determined.\n",
    "\n",
    "4. __Collect the Data__: Once you have finalized your plan, you can implement your data collection strategy and start collecting data. You can store and organize your data in your DMP. Be sure to stick to your plan and check on its progress regularly. It may be useful to create a schedule for when you will check in with how your data collection is proceeding, especially if you are collecting data continuously. You may want to make updates to your plan as conditions change and you get new information.\n",
    "\n",
    "5. __Analyze the Data and Implement Your Findings__: Once you’ve collected all of your data, it’s time to analyze it and organize your findings. The analysis phase is crucial because it turns raw data into valuable insights that you can use to enhance your marketing strategies, products and business decisions. You can use the analytics tools built into our DMP to help with this step. Once you’ve uncovered the patterns and insights in your data, you can implement the findings to improve your business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f60a2",
   "metadata": {},
   "source": [
    "__Ans 4__:\n",
    "- Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and data exploration until you get clear data. Ramifications or major issues in machine learning are:\n",
    "\n",
    "1. Five practical issues in machine learning and the business implications Data quality.\n",
    "2. Machine learning systems rely on data.\n",
    "3. The complexity and quality trade-off.\n",
    "4. Sampling bias in data.\n",
    "5. Changing expectations and concept drift.\n",
    "6. Monitoring and maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250cc8a",
   "metadata": {},
   "source": [
    "__Ans 5__:\n",
    "-  Various approaches to categorical data exploration are:\n",
    "\n",
    "1. __Unique value count__: One of the first things which can be useful during data exploration is to see how many unique values are there in categorical columns.\n",
    "2. __Frequency Count__: Frequency count is finding how frequent individual values occur in column.\n",
    "3. __Variance__: Variance gives a good indication how the values are spread.\n",
    "4. __Pareto Analysis__: Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration.\n",
    "5. __Histogram__: Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data.\n",
    "6. __Correlation Heat-map between all numeric columns__: The term correlation refers to a mutual relationship or association between two things.\n",
    "7. __Pearson Correlation and Trend between two numeric columns__: Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns.\n",
    "8. __Outlier overview__: Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36085a89",
   "metadata": {},
   "source": [
    "__Ans 6__:\n",
    "- Even in a Well-Designed & Controlled study, Missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.\n",
    "\n",
    "1. Real-world data collection has its own set of problems, It is often very messy which includes missing data, presence of outliers, unstructured manner, etc.\n",
    "2. Before looking for any insights from the data, we have to first perform preprocessing tasks which then only allow us to use that data for further observation and train our machine learning model.\n",
    "3. Missing value in a dataset is a very common phenomenon in the reality.\n",
    "4. Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "5. Most of the algorithms can’t handle missing data, thus you need to act in some way to simply not let your code crash. So, let’s begin with the methods to solve the problem.\n",
    "6. Methods for dealing with missing values. The popular methods which are used by the machine learning community to handle the missing value for categorical variables in the dataset are as follows: Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7e715",
   "metadata": {},
   "source": [
    "__Ans7__:\n",
    "-  The Various Methods for dealing with missing data values are:\n",
    "\n",
    "1. __Delete the observations__: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "2. __Replace missing values with the most frequent value__: You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "3. __Develop a model to predict missing values__: One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf636e",
   "metadata": {},
   "source": [
    "__Ans 8__:\n",
    "The Various Data Pre-Processing Techniques are:\n",
    "\n",
    "- __Data Cleaning__: The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "-  __Missing Data__: This situation arises when some data is missing in the data. It can be handled in various ways. Some of them are:\n",
    "2. __Ignore the tuples__: This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.\n",
    "3. __Fill the Missing values__: There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "- __Noisy Data__: Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways :\n",
    "1. __Binning Method__: This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task.\n",
    "2. __Regression__: Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple independent variables).\n",
    "3. __Clustering__: This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters.\n",
    "- __Data Reduction__ : Since data mining is a technique that is used to handle huge amount of data. While working with huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs. The various steps to data reduction are:\n",
    "1. __Data Cube Aggregation__: Aggregation operation is applied to data for the construction of the data cube.\n",
    "2. __Attribute Subset Selection__: The highly relevant attributes should be used, rest all can be discarded. For performing attribute selection, one can use level of significance and p- value of the attribute.the attribute having p-value greater than significance level can be discarded.\n",
    "3. __Numerosity Reduction__: This enable to store the model of data instead of whole data, for example: Regression Models.\n",
    "4. __Dimensionality Reduction__: This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Component Analysis).          \n",
    "Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc97ef3",
   "metadata": {},
   "source": [
    "__Ans 9__:\n",
    "- The following is the brief notes on the following topics:\n",
    "\n",
    "- __What is the IQR? What criteria are used to assess it?__\n",
    "1. Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "2. Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3\n",
    "3. The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "- __Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?__\n",
    "1. minimum is the minimum value in the dataset\n",
    "2. maximum is the maximum value in the dataset.\n",
    "3. So the difference between the two tells us about the range of dataset.\n",
    "4. The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).\n",
    "4. Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "5. Q3 is the third quartile of the data, i.e., to say 75%\n",
    "6. When the data is left skewed, lower whisker will be longer than upper whisker.\n",
    "7. To detect the outliers this method is used, we define a new range, let’s call it decision range, and any data point lying outside this range is considered as outlier and is accordingly dealt with. The range is as given below: Lower Bound: (Q1 - 1.5 * IQR)Upper Bound: (Q3 + 1.5 * IQR)\n",
    "8. The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
